{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression \n",
    "\n",
    "This last task will cover the logistic regression problem. And will actually be the easisest one to implement. Why? Because you did everthing before. As seen in the lab notes the steps and optimization are verry similar ot the linear regression problem. There are only a few changes to make nameley the loss function, hyposis and a small update to the gradient computation. We will also add a new metric at the end to evaluate the performce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Simalr to the last time we are going to load the data and process it in the same manner, meaning:\n",
    "\n",
    "- Split the dataset into training and test sets.\n",
    "- Normalize the data based on the training set.\n",
    "- Add a bias term (a column of ones) to the design\n",
    "\n",
    "The number of features may deffer and the meaning of them but the design matrix will be procesed in the same manner. The target variable is now descrete but does not influence this step. \n",
    "\n",
    "### 1.1 Load the Brest Cancer Dataset\n",
    "\n",
    "We will use the `load_breast_cancer` dataset from `sklearn`. The cell below loads the dataset and displays the feature names.\n",
    "\n",
    "### 1.2 Data Preprocessing\n",
    "\n",
    "To evaluate the performance of our linear regression model, we need to split the data into a **training set** and a **test set**. We will:\n",
    "- Learn the model on the training set.\n",
    "- Test the model on the test set.\n",
    "\n",
    "#### Bias Trick:\n",
    "As in the PLA and Linear Regression model, we will take advantage of the **bias trick** by adding a column of ones to the feature matrix.\n",
    "\n",
    "#### Normalization:\n",
    "Since the features in the dataset have different ranges, we need to normalize the data. The normalization is performed using the following formula:\n",
    "\n",
    "$$\n",
    "x = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mu $ is the mean of each feature.\n",
    "- $ \\sigma $ is the standard deviation of each feature.\n",
    "\n",
    "**Note:** The normalization parameters $ \\mu $ and $ \\sigma $ are computed **only from the training set** and then used to normalize both the training and test sets.\n",
    "\n",
    "#### Steps:\n",
    "- Split the dataset into training and test sets.\n",
    "- Normalize the data based on the training set.\n",
    "- Add a bias term (a column of ones) to the design\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "feature_names = data.feature_names\n",
    "\n",
    "print(f'The dataset has {X.shape[0]} samples and {X.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataset using histogram\n",
    "labels = ['Benign','Malignant']\n",
    "population = [np.sum(y),np.sum(y==0)]\n",
    "y_pos = np.arange(len(labels))\n",
    "barlist = plt.bar(y_pos, population, align='center',width=0.3)\n",
    "plt.xticks(y_pos, labels)\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Breast wisconsin dataset.')\n",
    "barlist[1].set_color('r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispalay the min max value of each feature\n",
    "for i in range(X.shape[1]):\n",
    "    print(f\"{feature_names[i]}: min={np.min(X[:,i])}, max={np.max(X[:,i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    #Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #Output: X: a numpy array of shape (N, D+1), where a column of ones is concatenated to the input array X\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "    \n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return X\n",
    "\n",
    "def normalize(X, mean=None, std=None):\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        mean: a numpy array of shape (D,) containing the mean of each feature\n",
    "    #        std: a numpy array of shape (D,) containing the standard deviation of each feature\n",
    "    # Output: X: a numpy array of shape (N, D), where each feature is normalized by subtracting the mean and dividing by the standard deviation\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "    X = (X - mean) / std\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return X\n",
    "\n",
    "def split_data(X, y, ratio=0.8):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #        ratio: a float number between 0 and 1, representing the ratio of training data\n",
    "    # Output: X_train: a numpy array of shape (N_train, D), containing the training data\n",
    "    #         y_train: a numpy array of shape (N_train,), containing the target for each training sample\n",
    "    #         X_test: a numpy array of shape (N_test, D), containing the testing data\n",
    "    #         y_test: a numpy array of shape (N_test,), containing the target for each testing sample\n",
    "\n",
    "    X_train, y_train, X_test, y_test = None, None, None, None\n",
    "    num_samples = None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "   \n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def prepocess_data(X, y, ratio=0.8):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #        ratio: a float number between 0 and 1, representing the ratio of training data\n",
    "    # Output: X_train: a numpy array of shape (N_train, D+1), containing the training data\n",
    "    #         y_train: a numpy array of shape (N_train,), containing the target for each training sample\n",
    "    #         X_test: a numpy array of shape (N_test, D+1), containing the testing data\n",
    "    #         y_test: a numpy array of shape (N_test,), containing the target for each testing sample\n",
    "\n",
    "    X_train, y_train, X_test, y_test = None, None, None, None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "    # Split the data\n",
    "\n",
    "\n",
    "    # Compute the mean and std of the training data\n",
    "   \n",
    "\n",
    "    # Normalize the training data\n",
    "\n",
    "    # Add intercept to both training and testing data\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing on the data\n",
    "X_train, y_train, X_test, y_test = prepocess_data(X, y)\n",
    "\n",
    "print('The shape of the training set is:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('The shape of the test set is:')\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation \n",
    "\n",
    "In this section, we will implement the essential components of Logistic Regression.\n",
    "\n",
    "### 2.1 Implement the Sigmoid Function\n",
    "Implement the `sigmoid` function, which takes as input a NumPy array or scalar and returns the sigmoid of the input element wise.\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "### 2.2 Implement the Logistic Regression Hypothesis\n",
    "\n",
    "Implement the `predict` method. This method should take as input:\n",
    "- A NumPy array of features\n",
    "- A corresponding set of weights \n",
    "\n",
    "It should return the predicted values (i.e., the hypothesis) for the input data, using the logistic regression model.\n",
    "\n",
    "### 2.3 Implement the Loss Function\n",
    "\n",
    "Implement the `compute_loss` function using Binary Cross-Entropy (BCE) loss. \n",
    "\n",
    "The BCE loss function is defined as:\n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^{m} \\left( y_i \\log h(\\mathbf{x}_i) + (1 - y^{(i)}) \\log \\left( 1 - h(\\mathbf{x}_i) \\right) \\right).\n",
    "$$\n",
    "\n",
    "where $ y_i $ are the true labels, and $h(\\mathbf{x}_i)$ are the predicted probabilities.\n",
    "\n",
    "### 2.4 Implement the Gradient Computation\n",
    "\n",
    "Implement the `compute_gradient` function to calculate the gradients needed for updating the model weights. \n",
    "\n",
    "Hint: The gradient computation is similar to that in linear regression, but with the logistic regression hypothesis instead.\n",
    "\n",
    "Note: The implementation should be done in vectorized form. (Bached implementation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "\n",
    "    # Input: x: a numpy array\n",
    "    # out: rez: a numpy array containing the sigmoid of the input array\n",
    "\n",
    "    out = None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def predict(X, w):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        w: a numpy array of shape (D,), containing the weights of the linear model\n",
    "    # Output: pred: a numpy array of shape (N,), containing the predicted values for the input data\n",
    "\n",
    "    pred = None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return pred\n",
    "\n",
    "def compute_loss(X, y, w):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #        w: a numpy array of shape (D,), containing the weights of the linear model\n",
    "    # Output: loss: a float number representing the average loss\n",
    "\n",
    "    loss = None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(X, y, w):\n",
    "\n",
    "    grad = None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return grad\n",
    "\n",
    "def compute_accuracy(X, y, w, threshold=0.5):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #        w: a numpy array of shape (D,), containing the weights of the linear model\n",
    "    #        threshold: a float number representing the threshold for the classification\n",
    "    # Output: acc: a float number representing the accuracy of the model\n",
    "\n",
    "    acc = None\n",
    "\n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and Testing Script\n",
    "\n",
    "The training and testing scripts are similar to those used in linear regression. However, for logistic regression, you should also add the accuracy metric to both the train and test functions.\n",
    "\n",
    "### 3.1 Implement the Train Function\n",
    "\n",
    "Implement the `train` function, which optimizes the model parameters. \n",
    "\n",
    "In addition to optimizing the weights using logistic regression, make sure to:\n",
    "- Compute and store the accuracy for each training step.\n",
    "- Return the list of computed accuracies alongside the optimized weights.\n",
    "\n",
    "### 3.2 Implement the Test Function\n",
    "\n",
    "Implement the `test` function to evaluate the model on unseen data.\n",
    "\n",
    "For this function:\n",
    "- Compute the loss on the test data.\n",
    "- Compute and return the accuracy of the model on the test data along with the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, w, alpha, no_iterations):\n",
    "\n",
    "    # Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #        y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #        w: a numpy array of shape (D,), containing the weights of the linear model\n",
    "    #        alpha: a float number representing the learning rate\n",
    "    #        no_iterations: an integer representing the number of iterations\n",
    "    # Output: w: a numpy array of shape (D,), the weights of the trained linear model\n",
    "    #         losses: a list of floats containing the loss at each update\n",
    "    #         accuraces: a list of floats containing the accuracy at each update\n",
    "\n",
    "    losses = []\n",
    "    accuraces = []\n",
    "\n",
    "    for step in range(no_iterations):\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "\n",
    "    return w, losses, accuraces\n",
    "\n",
    "def test(X, y, w):\n",
    "\n",
    "    #Input: X: a numpy array of shape (N, D), where N is the number of samples and D is the number of features\n",
    "    #       y: a numpy array of shape (N,), containing the target for each sample\n",
    "    #       w: a numpy array of shape (D,), containing the weights of the linear model\n",
    "    #Output: loss: a float number representing the loss of the model\n",
    "    #        accuracy: a float number representing the accuracy of the model\n",
    "    \n",
    "    loss, accuracy = None, None\n",
    "    \n",
    "    ########## Your code goes here ##########\n",
    "\n",
    "    \n",
    "\n",
    "    ######################################### \n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(X_train.shape[1])\n",
    "alpha = 0.003\n",
    "\n",
    "w, losses_train, accuracies_train = train(X_train, y_train, w, alpha, no_iterations=200)\n",
    "\n",
    "loss_test, accuracy_test = test(X_test, y_test, w)\n",
    "\n",
    "print(f'Stochastic Gradient Descent loss on test: {loss_test / X_test.shape[0]}')\n",
    "print(f'Stochastic Gradient Descent accuracy on test: {accuracy_test}')\n",
    "\n",
    "#Plot side by side the losses of the two algorithms should be 2 difffetn figures\n",
    "plt.figure()\n",
    "plt.plot(losses_train)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Gradient Descent')\n",
    "plt.show()\n",
    "\n",
    "#display the accuracy of the logistic regression on the training set\n",
    "plt.figure()\n",
    "plt.plot(accuracies_train)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Gradient Descent')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
